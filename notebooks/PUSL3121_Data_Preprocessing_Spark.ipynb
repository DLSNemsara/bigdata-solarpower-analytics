{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark py4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deyA9JQoe8D0",
        "outputId": "9e7f0109-853a-4baa-c2c5-dc2a59715759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"BigDataAnalytics\").getOrCreate()\n",
        "\n",
        "# Start time measurement\n",
        "start_time = time.time()"
      ],
      "metadata": {
        "id": "7iFZi7kvfPLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oosYyjnxetmR",
        "outputId": "992f2ea3-4843-4f91-e9d1-89edcee3f46f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|          DATE_TIME|\n",
            "+-------------------+\n",
            "|2020-05-15 00:00:00|\n",
            "|2020-05-15 00:15:00|\n",
            "|2020-05-15 00:30:00|\n",
            "|2020-05-15 00:45:00|\n",
            "|2020-05-15 01:00:00|\n",
            "+-------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------------------+\n",
            "|          DATE_TIME|\n",
            "+-------------------+\n",
            "|2020-05-15 00:00:00|\n",
            "|2020-05-15 00:00:00|\n",
            "|2020-05-15 00:00:00|\n",
            "|2020-05-15 00:00:00|\n",
            "|2020-05-15 00:00:00|\n",
            "+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, to_timestamp\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"BigDataAnalytics\").getOrCreate()\n",
        "\n",
        "# Load datasets\n",
        "weather_data = spark.read.csv(\"/content/Plant_1_Weather_Sensor_Data.csv\", header=True, inferSchema=True)\n",
        "generation_data = spark.read.csv(\"/content/Plant_1_Generation_Data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Convert DATE_TIME column to standard format\n",
        "weather_data = weather_data.withColumn(\"DATE_TIME\", to_timestamp(col(\"DATE_TIME\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
        "generation_data = generation_data.withColumn(\"DATE_TIME\", to_timestamp(col(\"DATE_TIME\"), \"dd-MM-yyyy HH:mm\"))\n",
        "\n",
        "# Verify the change by showing the first few rows\n",
        "weather_data.select(\"DATE_TIME\").show(5)\n",
        "generation_data.select(\"DATE_TIME\").show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loading â€“ Read and combine generation & weather datasets."
      ],
      "metadata": {
        "id": "YMa7fo_-nATA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5eezTFtetml",
        "outputId": "9e0d4db0-cc3f-47ec-d45a-24d73a0da1cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+--------+---------------------+--------+--------+-----------+-----------+------------------+-------------------+------------------+-----------+\n",
            "|          DATE_TIME|PLANT_ID|GENERATION_SOURCE_KEY|DC_POWER|AC_POWER|DAILY_YIELD|TOTAL_YIELD|WEATHER_SOURCE_KEY|AMBIENT_TEMPERATURE|MODULE_TEMPERATURE|IRRADIATION|\n",
            "+-------------------+--------+---------------------+--------+--------+-----------+-----------+------------------+-------------------+------------------+-----------+\n",
            "|2020-05-15 00:00:00| 4135001|      1BY6WEcLGh8j5v7|     0.0|     0.0|        0.0|  6259559.0|   HmiyD2TTLFNqkNe| 25.184316133333333|        22.8575074|        0.0|\n",
            "|2020-05-15 00:00:00| 4135001|      1IF53ai7Xc0U56Y|     0.0|     0.0|        0.0|  6183645.0|   HmiyD2TTLFNqkNe| 25.184316133333333|        22.8575074|        0.0|\n",
            "|2020-05-15 00:00:00| 4135001|      3PZuoBAID5Wc2HD|     0.0|     0.0|        0.0|  6987759.0|   HmiyD2TTLFNqkNe| 25.184316133333333|        22.8575074|        0.0|\n",
            "|2020-05-15 00:00:00| 4135001|      7JYdWkrLSPkdwr4|     0.0|     0.0|        0.0|  7602960.0|   HmiyD2TTLFNqkNe| 25.184316133333333|        22.8575074|        0.0|\n",
            "|2020-05-15 00:00:00| 4135001|      McdE0feGgRqW7Ca|     0.0|     0.0|        0.0|  7158964.0|   HmiyD2TTLFNqkNe| 25.184316133333333|        22.8575074|        0.0|\n",
            "+-------------------+--------+---------------------+--------+--------+-----------+-----------+------------------+-------------------+------------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Rename SOURCE_KEY in both datasets before merging\n",
        "generation_data = generation_data.withColumnRenamed(\"SOURCE_KEY\", \"GENERATION_SOURCE_KEY\")\n",
        "weather_data = weather_data.withColumnRenamed(\"SOURCE_KEY\", \"WEATHER_SOURCE_KEY\")\n",
        "\n",
        "# Merge datasets on DATE_TIME and PLANT_ID using join\n",
        "merged_data = generation_data.join(weather_data, on=[\"DATE_TIME\", \"PLANT_ID\"], how=\"inner\")\n",
        "\n",
        "# Show the first few rows of the merged data\n",
        "merged_data.show(5)\n",
        "\n",
        "# Save the merged dataset (saving in the same directory as initial data)\n",
        "merged_data.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/content/Merged_Plant_Data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "df = merged_data  # Assign the merged_data DataFrame to df\n",
        "\n",
        "# Calculate the count of missing (null) values per column\n",
        "missing_values_count = df.select([F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns])\n",
        "\n",
        "missing_values_count.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY6dS2AikU4t",
        "outputId": "20155bde-3bf8-4c3a-81de-4f1aac1ff6b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+---------------------+--------+--------+-----------+-----------+------------------+-------------------+------------------+-----------+\n",
            "|DATE_TIME|PLANT_ID|GENERATION_SOURCE_KEY|DC_POWER|AC_POWER|DAILY_YIELD|TOTAL_YIELD|WEATHER_SOURCE_KEY|AMBIENT_TEMPERATURE|MODULE_TEMPERATURE|IRRADIATION|\n",
            "+---------+--------+---------------------+--------+--------+-----------+-----------+------------------+-------------------+------------------+-----------+\n",
            "|        0|       0|                    0|       0|       0|          0|          0|                 0|                  0|                 0|          0|\n",
            "+---------+--------+---------------------+--------+--------+-----------+-----------+------------------+-------------------+------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate basic statistics\n",
        "summary = df.select('DC_POWER', 'AC_POWER').describe()\n",
        "\n",
        "# Show summary statistics (count, mean, stddev, min, max)\n",
        "summary.show()\n",
        "\n",
        "# Calculate specific percentiles (25%, 50%, 75%)\n",
        "percentiles = [0.25, 0.5, 0.75]\n",
        "percentile_values = df.approxQuantile(['DC_POWER', 'AC_POWER'], percentiles, 0.01)\n",
        "\n",
        "# Displaying the percentiles\n",
        "print(\"Percentiles (25%, 50%, 75%) for DC_POWER and AC_POWER:\")\n",
        "for i, col_name in enumerate(['DC_POWER', 'AC_POWER']):\n",
        "    print(f\"{col_name}: 25% = {percentile_values[i][0]}, 50% (median) = {percentile_values[i][1]}, 75% = {percentile_values[i][2]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJvgC0CvnM14",
        "outputId": "00542927-b907-4316-d2b2-ffdf7d27ef82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+------------------+\n",
            "|summary|          DC_POWER|          AC_POWER|\n",
            "+-------+------------------+------------------+\n",
            "|  count|             68774|             68774|\n",
            "|   mean|3147.1774501376367| 307.7783754808176|\n",
            "| stddev|4036.4418255816813|394.39486476224266|\n",
            "|    min|               0.0|               0.0|\n",
            "|    max|         14471.125|           1410.95|\n",
            "+-------+------------------+------------------+\n",
            "\n",
            "Percentiles (25%, 50%, 75%) for DC_POWER and AC_POWER:\n",
            "DC_POWER: 25% = 0.0, 50% (median) = 330.25, 75% = 6201.714286\n",
            "AC_POWER: 25% = 0.0, 50% (median) = 31.9125, 75% = 607.5857143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count zero values in the respective columns\n",
        "zero_dc = df.filter(df[\"DC_POWER\"] == 0).count()\n",
        "zero_ac = df.filter(df[\"AC_POWER\"] == 0).count()\n",
        "zero_total_yield = df.filter(df[\"TOTAL_YIELD\"] == 0).count()\n",
        "zero_irradiation = df.filter(df[\"IRRADIATION\"] == 0).count()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Zero DC Power: {zero_dc} instances\")\n",
        "print(f\"Zero AC Power: {zero_ac} instances\")\n",
        "print(f\"Zero Total Yield: {zero_total_yield} instances\")\n",
        "print(f\"Zero Irradiation: {zero_irradiation} instances\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFA_J7GSnNUT",
        "outputId": "6510b2de-79a1-4907-e9a9-9d31e150ae80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero DC Power: 31951 instances\n",
            "Zero AC Power: 31951 instances\n",
            "Zero Total Yield: 0 instances\n",
            "Zero Irradiation: 30398 instances\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleansing â€“ Standardized DATE_TIME, removed redundant columns, and analyzed zero values."
      ],
      "metadata": {
        "id": "9vnGCvfzouxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, hour, when\n",
        "\n",
        "# Extract hour from DATE_TIME\n",
        "df = df.withColumn(\"Hour\", hour(col(\"DATE_TIME\")))\n",
        "\n",
        "# List of columns to check\n",
        "columns_to_check = [\"DC_POWER\", \"AC_POWER\", \"TOTAL_YIELD\", \"IRRADIATION\"]\n",
        "\n",
        "# Create conditions to check for zero values in each column\n",
        "zero_conditions = [when(col(c) == 0, 1).otherwise(0).alias(c) for c in columns_to_check]\n",
        "\n",
        "# Add the zero value check columns to the DataFrame\n",
        "df_with_zeros = df.select(\"Hour\", *zero_conditions)\n",
        "\n",
        "# Filter rows where any of the columns have zero values\n",
        "zero_summary = df_with_zeros.filter(\n",
        "    (col(\"DC_POWER\") == 1) | (col(\"AC_POWER\") == 1) |\n",
        "     (col(\"TOTAL_YIELD\") == 1) | (col(\"IRRADIATION\") == 1)\n",
        ")\n",
        "\n",
        "# Group by Hour and count the number of zero values in each column\n",
        "zero_count_by_hour = zero_summary.groupBy(\"Hour\").agg(\n",
        "    F.sum(\"DC_POWER\").alias(\"DC_POWER\"),\n",
        "    F.sum(\"AC_POWER\").alias(\"AC_POWER\"),\n",
        "    F.sum(\"TOTAL_YIELD\").alias(\"TOTAL_YIELD\"),\n",
        "    F.sum(\"IRRADIATION\").alias(\"IRRADIATION\")\n",
        ")\n",
        "\n",
        "# Show the result\n",
        "zero_count_by_hour.orderBy(\"Hour\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7u_DDSZoyo8",
        "outputId": "1f015be5-a107-4615-c346-a83694090637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+--------+-----------+-----------+\n",
            "|Hour|DC_POWER|AC_POWER|TOTAL_YIELD|IRRADIATION|\n",
            "+----+--------+--------+-----------+-----------+\n",
            "|   0|    2724|    2724|          0|       2724|\n",
            "|   1|    2726|    2726|          0|       2726|\n",
            "|   2|    2810|    2810|          0|       2810|\n",
            "|   3|    2812|    2812|          0|       2812|\n",
            "|   4|    2815|    2815|          0|       2815|\n",
            "|   5|    2707|    2707|          0|       2092|\n",
            "|   6|     138|     138|          0|          0|\n",
            "|   9|       2|       2|          0|          0|\n",
            "|  10|       2|       2|          0|          0|\n",
            "|  11|       6|       6|          0|          0|\n",
            "|  12|      19|      19|          0|          0|\n",
            "|  13|      27|      27|          0|          0|\n",
            "|  14|       6|       6|          0|          0|\n",
            "|  15|       1|       1|          0|          0|\n",
            "|  18|     903|     903|          0|        248|\n",
            "|  19|    2669|    2669|          0|       2587|\n",
            "|  20|    2924|    2924|          0|       2924|\n",
            "|  21|    2962|    2962|          0|       2962|\n",
            "|  22|    2948|    2948|          0|       2948|\n",
            "|  23|    2750|    2750|          0|       2750|\n",
            "+----+--------+--------+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have added a new column by extracting Hour from DATE_TIME.\n",
        "\n",
        "This will show:\n",
        "- A table with hours (0-23) as rows.\n",
        "- Counts of how many times each column had zero values at each hour.\n",
        "\n",
        "Based on the results, most zero values appear at night (0:00 - 6:00, 18:00 - 23:00) and during some early morning or late afternoon hours when sunlight is weaker. This is expected for a solar power plant because:\n",
        "- Nighttime (0:00 - 6:00, 19:00 - 23:00) â†’ No sunlight â†’ No power generation, no irradiation.\n",
        "- Early morning (6:00 - 9:00) & late afternoon (15:00 - 18:00) â†’ Low sunlight â†’ Some zeros, but minimal.\n",
        "- Daytime (10:00 - 14:00) â†’ Few zeros â†’ Power generation is good.\n",
        "\n",
        "What this means for data cleaning:\n",
        "1. Zeros at night (expected) â†’ No need to remove them.\n",
        "2. Zeros during daylight (rare but exists):\n",
        "- Might be due to temporary weather conditions (clouds, rain).\n",
        "- Might indicate sensor faults (unlikely, but possible).\n",
        "- Since few zeros appear between 9:00 - 15:00, itâ€™s not a major issue.\n"
      ],
      "metadata": {
        "id": "d3yzmAI1sYaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop GENERATION_SOURCE_KEY, WEATHER_SOURCE_KEY, and PLANT_ID columns\n",
        "merged_data = merged_data.drop(\"GENERATION_SOURCE_KEY\", \"WEATHER_SOURCE_KEY\", \"PLANT_ID\")\n",
        "\n",
        "# Show the first few rows of the merged data to confirm changes\n",
        "merged_data.show(5)\n",
        "\n",
        "# Save the cleaned dataset to CSV (overwrite mode, header included)\n",
        "merged_data.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/content/Cleaned_Plant_Data.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvY8rZbipiiG",
        "outputId": "5eef7b44-3ac4-4592-b1a3-7850ea171c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+--------+--------+-----------+-----------+-------------------+------------------+-----------+\n",
            "|          DATE_TIME|DC_POWER|AC_POWER|DAILY_YIELD|TOTAL_YIELD|AMBIENT_TEMPERATURE|MODULE_TEMPERATURE|IRRADIATION|\n",
            "+-------------------+--------+--------+-----------+-----------+-------------------+------------------+-----------+\n",
            "|2020-05-15 00:00:00|     0.0|     0.0|        0.0|  6259559.0| 25.184316133333333|        22.8575074|        0.0|\n",
            "|2020-05-15 00:00:00|     0.0|     0.0|        0.0|  6183645.0| 25.184316133333333|        22.8575074|        0.0|\n",
            "|2020-05-15 00:00:00|     0.0|     0.0|        0.0|  6987759.0| 25.184316133333333|        22.8575074|        0.0|\n",
            "|2020-05-15 00:00:00|     0.0|     0.0|        0.0|  7602960.0| 25.184316133333333|        22.8575074|        0.0|\n",
            "|2020-05-15 00:00:00|     0.0|     0.0|        0.0|  7158964.0| 25.184316133333333|        22.8575074|        0.0|\n",
            "+-------------------+--------+--------+-----------+-----------+-------------------+------------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this code does:\n",
        "1. Adds a new column: Power_Efficiency = AC_POWER / DC_POWER\n",
        "\n",
        "2. Handles division by zero by filling NaN values with 0.\n",
        "\n",
        "This is a column we additionaly added using columns AC_POWER & DC_POWER"
      ],
      "metadata": {
        "id": "hU6VVmhn22ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load cleaned dataset (infer schema to avoid string type issues)\n",
        "df = spark.read.csv(\"Cleaned_Plant_Data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Compute Power Efficiency safely (avoid division by zero)\n",
        "df = df.withColumn(\n",
        "    \"Power_Efficiency\",\n",
        "    when(col(\"DC_POWER\") != 0, col(\"AC_POWER\") / col(\"DC_POWER\")).otherwise(0)\n",
        ")\n",
        "\n",
        "# Save the updated dataset as a single CSV file\n",
        "df.coalesce(1).write.csv(\"Cleaned_Plant_Data_Spark\", header=True, mode=\"overwrite\")\n",
        "\n",
        "print(\"Power_Efficiency column added successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LgE5rdMvELX",
        "outputId": "8d2e53b4-e3c9-40df-9fdd-ee9430ffc1d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Power_Efficiency column added successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate execution time"
      ],
      "metadata": {
        "id": "QTbE4pweYUMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# End time measurement\n",
        "end_time = time.time()\n",
        "\n",
        "# Print execution time\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Spark preprocessing execution time: {execution_time:.5f} seconds\")\n",
        "\n",
        "# Save execution time to CSV\n",
        "execution_time_data = pd.DataFrame({'Framework': ['Spark'], 'Execution Time (seconds)': [execution_time]})\n",
        "execution_time_data.to_csv('/content/execution_time_spark.csv', index=False)  # Save CSV file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rQjvZTGSQjo",
        "outputId": "6eb94948-97eb-4c83-b899-0cfb6eb252cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark preprocessing execution time: 19.74609 seconds\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}